{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x_data = [1,2,3]\n",
    "y_data = [1,2,3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "W = tf.Variable(tf.random_uniform([1],-1,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "b = tf.Variable(tf.random_uniform([1],-1,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X = tf.placeholder(tf.float32,name = 'X')\n",
    "Y = tf.placeholder(tf.float32,name = 'Y')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "hypothesis = W*X+b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cost = tf.reduce_mean(tf.square(hypothesis-Y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate = 0.1)\n",
    "train_op = optimizer.minimize(cost)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 3.96315 [ 0.51826525] [ 1.31726301]\n",
      "1 0.279882 [ 0.44097909] [ 1.24650431]\n",
      "2 0.224839 [ 0.46413022] [ 1.22081184]\n",
      "3 0.21366 [ 0.47595063] [ 1.19099736]\n",
      "4 0.203505 [ 0.48866445] [ 1.16241765]\n",
      "5 0.193838 [ 0.5009439] [ 1.13446832]\n",
      "6 0.184631 [ 0.51294225] [ 1.10719705]\n",
      "7 0.175861 [ 0.52465069] [ 1.08058071]\n",
      "8 0.167507 [ 0.53607774] [ 1.05460429]\n",
      "9 0.159551 [ 0.54723012] [ 1.02925229]\n",
      "10 0.151972 [ 0.55811441] [ 1.00450981]\n",
      "11 0.144753 [ 0.56873703] [ 0.98036206]\n",
      "12 0.137877 [ 0.57910436] [ 0.95679486]\n",
      "13 0.131328 [ 0.58922231] [ 0.93379414]\n",
      "14 0.12509 [ 0.59909713] [ 0.91134638]\n",
      "15 0.119148 [ 0.60873461] [ 0.88943827]\n",
      "16 0.113488 [ 0.61814034] [ 0.86805677]\n",
      "17 0.108097 [ 0.62731993] [ 0.84718925]\n",
      "18 0.102963 [ 0.63627893] [ 0.82682341]\n",
      "19 0.098072 [ 0.64502257] [ 0.80694717]\n",
      "20 0.0934135 [ 0.65355599] [ 0.78754872]\n",
      "21 0.0889763 [ 0.66188425] [ 0.76861656]\n",
      "22 0.0847499 [ 0.67001235] [ 0.75013953]\n",
      "23 0.0807242 [ 0.67794508] [ 0.73210669]\n",
      "24 0.0768897 [ 0.68568701] [ 0.71450728]\n",
      "25 0.0732374 [ 0.69324291] [ 0.69733101]\n",
      "26 0.0697586 [ 0.70061713] [ 0.68056768]\n",
      "27 0.0664449 [ 0.70781404] [ 0.66420728]\n",
      "28 0.0632888 [ 0.71483803] [ 0.64824021]\n",
      "29 0.0602825 [ 0.72169316] [ 0.63265699]\n",
      "30 0.057419 [ 0.72838336] [ 0.61744833]\n",
      "31 0.0546916 [ 0.73491287] [ 0.60260534]\n",
      "32 0.0520937 [ 0.74128538] [ 0.58811915]\n",
      "33 0.0496192 [ 0.74750465] [ 0.57398117]\n",
      "34 0.0472623 [ 0.75357455] [ 0.56018311]\n",
      "35 0.0450173 [ 0.75949842] [ 0.54671669]\n",
      "36 0.0428789 [ 0.76527989] [ 0.53357399]\n",
      "37 0.0408421 [ 0.77092236] [ 0.52074724]\n",
      "38 0.0389021 [ 0.7764293] [ 0.50822884]\n",
      "39 0.0370543 [ 0.78180373] [ 0.49601135]\n",
      "40 0.0352941 [ 0.78704906] [ 0.48408759]\n",
      "41 0.0336176 [ 0.7921682] [ 0.47245044]\n",
      "42 0.0320208 [ 0.79716438] [ 0.46109307]\n",
      "43 0.0304998 [ 0.8020404] [ 0.45000869]\n",
      "44 0.029051 [ 0.80679923] [ 0.43919078]\n",
      "45 0.0276711 [ 0.81144363] [ 0.42863292]\n",
      "46 0.0263567 [ 0.81597638] [ 0.41832888]\n",
      "47 0.0251047 [ 0.82040018] [ 0.40827256]\n",
      "48 0.0239122 [ 0.82471764] [ 0.39845797]\n",
      "49 0.0227764 [ 0.82893127] [ 0.3888793]\n",
      "50 0.0216945 [ 0.83304369] [ 0.37953094]\n",
      "51 0.020664 [ 0.83705717] [ 0.37040728]\n",
      "52 0.0196824 [ 0.84097421] [ 0.36150295]\n",
      "53 0.0187475 [ 0.84479713] [ 0.35281268]\n",
      "54 0.017857 [ 0.84852803] [ 0.34433126]\n",
      "55 0.0170087 [ 0.85216933] [ 0.33605379]\n",
      "56 0.0162008 [ 0.85572308] [ 0.3279753]\n",
      "57 0.0154313 [ 0.85919142] [ 0.32009101]\n",
      "58 0.0146983 [ 0.86257637] [ 0.31239623]\n",
      "59 0.0140001 [ 0.86587995] [ 0.30488643]\n",
      "60 0.0133351 [ 0.86910415] [ 0.29755718]\n",
      "61 0.0127016 [ 0.87225074] [ 0.29040408]\n",
      "62 0.0120983 [ 0.87532175] [ 0.28342298]\n",
      "63 0.0115236 [ 0.87831897] [ 0.27660969]\n",
      "64 0.0109763 [ 0.88124406] [ 0.26996017]\n",
      "65 0.0104549 [ 0.88409889] [ 0.2634705]\n",
      "66 0.00995827 [ 0.88688511] [ 0.25713685]\n",
      "67 0.00948524 [ 0.88960427] [ 0.25095543]\n",
      "68 0.00903468 [ 0.89225817] [ 0.24492265]\n",
      "69 0.00860552 [ 0.89484817] [ 0.23903486]\n",
      "70 0.00819674 [ 0.89737594] [ 0.23328863]\n",
      "71 0.00780741 [ 0.89984298] [ 0.22768055]\n",
      "72 0.00743655 [ 0.90225071] [ 0.22220726]\n",
      "73 0.0070833 [ 0.9046005] [ 0.21686554]\n",
      "74 0.00674685 [ 0.90689385] [ 0.21165223]\n",
      "75 0.00642636 [ 0.909132] [ 0.20656423]\n",
      "76 0.0061211 [ 0.91131645] [ 0.2015986]\n",
      "77 0.00583034 [ 0.91344833] [ 0.19675229]\n",
      "78 0.0055534 [ 0.91552895] [ 0.1920225]\n",
      "79 0.00528961 [ 0.91755962] [ 0.18740642]\n",
      "80 0.00503835 [ 0.91954136] [ 0.18290128]\n",
      "81 0.00479902 [ 0.92147553] [ 0.17850447]\n",
      "82 0.00457106 [ 0.92336321] [ 0.17421335]\n",
      "83 0.00435394 [ 0.92520553] [ 0.17002538]\n",
      "84 0.00414713 [ 0.9270035] [ 0.16593808]\n",
      "85 0.00395012 [ 0.92875832] [ 0.16194905]\n",
      "86 0.0037625 [ 0.93047094] [ 0.1580559]\n",
      "87 0.00358378 [ 0.93214244] [ 0.15425636]\n",
      "88 0.00341355 [ 0.93377364] [ 0.1505481]\n",
      "89 0.0032514 [ 0.93536568] [ 0.14692901]\n",
      "90 0.00309695 [ 0.93691945] [ 0.14339693]\n",
      "91 0.00294985 [ 0.93843585] [ 0.13994977]\n",
      "92 0.00280972 [ 0.93991584] [ 0.13658547]\n",
      "93 0.00267626 [ 0.94136024] [ 0.13330205]\n",
      "94 0.00254914 [ 0.94276989] [ 0.13009755]\n",
      "95 0.00242805 [ 0.94414562] [ 0.12697008]\n",
      "96 0.00231272 [ 0.94548833] [ 0.12391782]\n",
      "97 0.00220286 [ 0.9467988] [ 0.12093893]\n",
      "98 0.00209822 [ 0.94807768] [ 0.11803161]\n",
      "99 0.00199856 [ 0.94932592] [ 0.11519423]\n",
      "\n",
      "=== Test ===\n",
      "X:5,Y: [ 4.86182404]\n",
      "X:2.5,Y: [ 2.48850918]\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    for step in range(100):\n",
    "        _,cost_val = sess.run([train_op,cost],feed_dict = {X:x_data,Y:y_data})\n",
    "        print(step,cost_val,sess.run(W),sess.run(b))\n",
    "        \n",
    "    print('\\n=== Test ===')\n",
    "    print('X:5,Y:',sess.run(hypothesis,feed_dict = {X:5}))\n",
    "    print('X:2.5,Y:',sess.run(hypothesis,feed_dict = {X:2.5}))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
